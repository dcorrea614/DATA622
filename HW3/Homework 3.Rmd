---
title: "DATA 622 - Homework3 - Loan Approval Prediction"
author: "Group1: Diego Correa, Amanda Schettini, Soumya Ghosh & Atina Karim"
date: "October 10, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: default
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(rpart.plot)
library(ISLR)
library(tree)
library(DAAG)

```

## Background

For this assignment, we will be working with a dataset on loan approval status. The **'Loan_Status'** is the target variable here -

### Data Dictionary

![**Loan Approval Status Data Dictionary**](https://github.com/dcorrea614/DATA622/blob/main/HW3/Images/DataDictionary.PNG?raw=true)

### Problem Statement

1. As we begin working with the dataset, we will conduct a thorough exploratory data analysis. This step is necessary as we figure out which variables should be included in models. (10 points)
2. We will use the LDA algorithm to predict the loan approval status. This will include the walk through for the steps we took, and how we decided on the key variables. (40 points)
3. Use K-nearest neighbor (KNN) algorithm to predict the loan approval status variable. Please be sure to walk through the steps you took. This includes talking about what value for ‘k’ you settled on and why. (40 points)
4. Use Decision Trees to predict on loan approval status. (40 points)
5. Use Random Forests to predict on loan approval status. (40 points)
6. Model performance: Comparison of the models we settled on in problem # 2- 5. Comment on their relative performance. Which one would you prefer the most? Why? (5 points)

## Dataset

```{r warning=FALSE, message=FALSE}
dataset <- read.csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/HW3/Loan_approval.csv')
head(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")
```

### Descriptive Dataset Summary

```{r warning=FALSE, message=FALSE}
summary(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


## Pre-Processing

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r message=FALSE, warning=FALSE, fig.height=4}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(dataset, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(dataset, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`)) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())

```

```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
dataset_missing_counts <- sapply(dataset, function(x) sum(is.na(x))) %>% 
  sort(decreasing = TRUE) %>%
  as.data.frame() %>% rename('NA_Count' ='.') 

dataset_missing_counts <- dataset_missing_counts%>%
  mutate('Feature' = rownames(dataset_missing_counts))

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())
```

### Data Imputation 

```{r message=FALSE, warning=FALSE}
#transformation
#Loan_ID should be removed before imputing data
#mice uses all data to impute
#Transforming continuous variables by taking the log
dataset <- dataset %>%
  select(-'Loan_ID') %>%
  mutate(
    Gender = as.factor(Gender),
    Married = as.factor(Married),
    Dependents = as.factor(Dependents),
    Education = as.factor(Education),
    Self_Employed = as.factor(Self_Employed),
    Credit_History = as.factor(Credit_History),
    Property_Area = as.factor(Property_Area),
    Loan_Status = as.factor(Loan_Status)
  #  ApplicantIncome = log(ApplicantIncome),
  #  CoapplicantIncome = log(CoapplicantIncome),
   # LoanAmount = log(LoanAmount)
    #Loan_Amount_Term = log(Loan_Amount_Term)
  )
```



```{r message=FALSE, warning=FALSE}
#imputation by using the random forest method ('rf')
init <- mice(dataset, maxit = 0)
predM <- init$predictorMatrix
set.seed(123)
imputed <- mice(dataset, method = 'rf', predictorMatrix = predM, m=5)
```

```{r message=FALSE, warning=FALSE}
dataset <- complete(imputed)
summary(dataset)
```

```{r message=FALSE, warning=FALSE} 
# none of the variables meet the condition to be a degenerate feature
nearZeroVar(dataset)
```

## Exploratory Data Analysis

### Categorical Variables

We will evaluate loan status by the categorical variables. Blue bars indicate total amount of loan approvals relative to the pink bars, which are loan denials.

```{r message=FALSE, warning=FALSE, fig.height=6, fig.width=10}
cat_vars <- dataset %>%
  dplyr::select(-c('ApplicantIncome', 'CoapplicantIncome','LoanAmount','Loan_Amount_Term')) %>%
  gather(key = 'predictor_variable', value = 'value', -Loan_Status)

# Plot and print a histogram for each predictor variable.
ggplot(cat_vars) +
  geom_histogram(aes(x = value, fill = Loan_Status),stat='count', bins = 30) +
  labs(title = 'Distributions of Categorical Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)
```


### Continuous Variables

```{r message=FALSE, warning=FALSE, fig.width=10}
cont_vars <- dataset %>%
  dplyr::select(ApplicantIncome, CoapplicantIncome, LoanAmount, Loan_Amount_Term, Loan_Status) %>%
  gather(key = 'predictor_variable', value = 'value', -Loan_Status)

# Plot and print a histogram for each predictor variable.
ggplot(cont_vars) +
  geom_histogram(aes(x = value, y = ..density.., fill = Loan_Status), bins = 30) +
  labs(title = 'Log Distributions of Continuous Variables') +
  scale_x_log10() +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 2)
```

### Observations:

 - More males have applied for loans than females and and they also have a higher rate of approval.
 - More married couples have applied for loans. 
 - Self employed individuals have applied for less loans which indicates salary earners apply for and obtain more loans. 
 - People with better credit history guidelines are more likely to get their loans approved as they have higher chances of paying back the loan on time.
 - People leaving in Semi-Urban area have most loan applications and have a higher rate of approval followed by urban and rural areas. Especially Rural loan applicants have a lower rate of loan approval.
 - An extremely high number of them go for a 360 months loan term. That’s pay back within a 15 years period.
 - People with *no dependents* tend to have applied for more loan applications
 - People with a graduate degree have applied for more loans than w/o a graduate degree and have much higher rate of loan approvals.
 
### Further Analysis
 
```{r message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Plot and print a histogram for a pair of predictor variables.
bp <- ggplot(dataset, aes(x = ApplicantIncome)) +
  geom_histogram(bins = 30, color = "darkblue", fill = "lightblue") +
  labs(title = 'Distributions of Applicant Income By Gender and Marriage Status') +
  theme(plot.title = element_text(hjust = 0.5)) 

bp + facet_grid(Gender ~ Married, labeller=label_both) 

```

```{r message=FALSE, warning=FALSE, fig.height=4, fig.width=8}
# Plot and print a histogram for each predictor variable.
bp <- ggplot(dataset, aes(x = ApplicantIncome)) +
  geom_histogram(bins = 30, color = "darkblue", fill = "lightblue") +
  labs(title = 'Distributions of Applicant Income By Education and Self-Employed') +
  theme(plot.title = element_text(hjust = 0.5)) 

bp + facet_grid(Education ~ Self_Employed, labeller=label_both) 

```

```{r message=FALSE, warning=FALSE, fig.height=5, fig.width=8}
# Plot and print a histogram for each predictor variable.
bp <- ggplot(dataset, aes(x = ApplicantIncome)) +
  geom_histogram(bins = 30, color = "darkblue", fill = "lightblue") +
  labs(title = 'Distributions of Applicant Income By Dependents and Property_Area') +
  theme(plot.title = element_text(hjust = 0.5)) 

bp + facet_grid(Dependents ~ Property_Area, labeller=label_both) 

```


## Feature Engineering

Converting all categorical variables present in the dataset to numeric codes -

```{r}
# Caret package dummyVars() to do one hot encoding
#dummy <- dummyVars(" ~ .", data=dataset)
#newdata <- data.frame(predict(dummy, newdata = dataset))

# Converting categorical variables to numeric
newdata <- dataset %>% mutate_if(is.factor, as.numeric)

```


```{r}
newdata <- newdata %>%
  mutate(
    ApplicantIncome = log(ApplicantIncome),
    CoapplicantIncome = log(CoapplicantIncome),
    LoanAmount = log(LoanAmount),
    Loan_Amount_Term = log(Loan_Amount_Term))

newdata <- newdata %>%
  mutate(CoapplicantIncome = ifelse(CoapplicantIncome < 0, 0, CoapplicantIncome))
    
head(newdata) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")
```


### Correlation Plot: Multicollinearity Check

```{r fig.height=6, fig.width=7, message=FALSE, warning=FALSE}
corrMatrix <- round(cor(newdata),4)

corrMatrix %>% corrplot(., method = "color", outline = T, addgrid.col = "darkgray", order="hclust", addrect = 4, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.0, cl.cex = 1.0, addCoef.col = "white", number.digits = 2, number.cex = 0.8, col = colorRampPalette(c("darkred","white","dodgerblue4"))(100))

```

Based on above plot, it can be concluded there is no multicollinearity present in the dataset. There are certain variable pairs like **LoanAmount & ApplicantIncome** and **Credit_History & Loan_Status** that have higher correlation due to obvious reasons.

## Model Building

### Splitting Data: Train/Test

```{r}
sample = sample.split(newdata$Loan_Status, SplitRatio = 0.75)
train = subset(newdata, sample == TRUE)
test = subset(newdata, sample == FALSE)

#head(train)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

#Creating seperate dataframe for 'Loan_Status' feature which is our target.
train.loan_labels <- train[,12]
test.loan_labels <- test[,12]

train1 <- train[,-12]
test1 <- test[,-12]
```

### Model1: LDA Algorithm

### Model2: K-nearest neighbor (KNN) algorithm

```{r}
#Find the number of observation
NROW(train$Loan_Status)
```

So, we have 460 observations in our training data set. The square root of 460 is around 21.45, therefore we’ll create two models. One with ‘K’ value as 21 and the other model with a ‘K’ value as 22.


```{r}
knn.21 <- knn(train=train1, test=test1, cl=train.loan_labels, k=21)
knn.22 <- knn(train=train1, test=test1, cl=train.loan_labels, k=22)

#Calculate the proportion of correct classification for k = 21, 22
ACC.21 <- 100 * sum(test.loan_labels == knn.21)/NROW(test.loan_labels)
ACC.22 <- 100 * sum(test.loan_labels == knn.22)/NROW(test.loan_labels)

# Print Accuracy Scores
ACC.21
ACC.22

# Check prediction against actual value in tabular form for k=21
table(knn.21 ,test.loan_labels)

# Check prediction against actual value in tabular form for k=22
confusionMatrix(table(knn.22 ,test.loan_labels))
```

#### Model Optimization

In order to improve the accuracy of the model, you can use n number of techniques such as the Elbow method and maximum percentage accuracy graph.  In the below code snippet, I’ve created a loop that calculates the accuracy of the KNN model for ‘K’ values ranging from 1 to 25. This way you can check which ‘K’ value will result in the most accurate model:

```{r}
i=1
k.optm=1
for (i in 1:27){
 knn.mod <- knn(train=train1, test=test1, cl=train.loan_labels, k=i)
 k.optm[i] <- 100 * sum(test.loan_labels == knn.mod)/NROW(test.loan_labels)
 k=i
 cat(k,'=',k.optm[i],'
')
}
```

#### Accuracy Plot

```{r}
#Accuracy plot
plot(k.optm, type="b", xlab="K- Value",ylab="Accuracy level")
```

### Model3: Decision Tree

Prior to building our decision tree - we will need to ensure that all categorical variables in our train and test set are coded as factors:

#### Converting Categorical Variables to Factors in Train
```{r}
train2 <- train %>%
  
  mutate(
    Gender = as.factor(Gender),
    Married = as.factor(Married),
    Dependents = as.factor(Dependents),
    Education = as.factor(Education),
    Self_Employed = as.factor(Self_Employed),
    Credit_History = as.factor(Credit_History),
    Property_Area = as.factor(Property_Area),
    Loan_Status = as.factor(Loan_Status))
```

#### Converting Categorical Variables to Factors in Test

```{r}
test2 <- test %>%
  
  mutate(
    Gender = as.factor(Gender),
    Married = as.factor(Married),
    Dependents = as.factor(Dependents),
    Education = as.factor(Education),
    Self_Employed = as.factor(Self_Employed),
    Credit_History = as.factor(Credit_History),
    Property_Area = as.factor(Property_Area),
    Loan_Status = as.factor(Loan_Status))
```

#### Initial Decision Tree

```{r}
dt <- rpart(Loan_Status ~ ., 
               data=train2, method="class")

rpart.plot(dt, nn=TRUE)
```

Here are some of the details of the tree:
```{r}
summary (dt)
```
Credit history and income seem to be some of the important predictors for loan approval.

#### Predicting on the Test Data

We are going to apply our tree to the test data and create a confusion table to evaluate the accuracy of the classifications.

```{r}
tree.pred = predict(dt,test2,type="class") #because we want to predict the class labels
# Confusion Tree
confusion(train2$Loan_Status, predict(dt, type = "class"))
```
Our model is 83% accurate. This tree was grown to full depth and therefore there might be too many variables. To achieve improved accuracy, we need to prune the tree using cross-validation:

```{r}
plotcp(dt)
dt$cptable
```
The plot above shows the cross validated errors against the complexity parameters.
The curve is at its lowest at 2, so we will prune our tree to a size of 2. At size 2, the error is ~0.638 and cp is 0.01157407

```{r}
prune_dt=prune(dt,cp=0.01157)
rpart.plot(prune_dt)
```

#### Predicting Pruned Tree on Test Data 

```{r}
Prune_pred <- predict(prune_dt, 
                   test2, 
                   type="class")

confusionMatrix(Prune_pred, test2$Loan_Status)
```
Seems like pruning the tree improved model accuracy to 84% with Credit History being the most significant factor in determining loan status.

However, more often than not, trees do not give very good prediction errors. Therefore, we will build out a random forest models which tend to outperform trees in terms of prediction and misclassification errors.

### Model4: Random Forest

```{r}
# Finding best mtry to use in random forest model by evaluating using the lowest OOB error
mtry <- randomForest::tuneRF(train[-12],train$Loan_Status, ntreeTry=500,
               stepFactor=1.5,improve=0.01, trace=TRUE, plot=TRUE)
best.m <- mtry[mtry[, 2] == min(mtry[, 2]), 1]
print(mtry)
print(best.m)
```

```{r}
# Using best mtry in model, plotting importance
set.seed(71)
rf <-randomForest(Loan_Status~.,data=train, mtry=best.m, importance=TRUE,ntree=500)
print(rf)
```

```{r}
#Evaluate variable importance
varImpPlot(rf)
```


## Model Performance Comparision

## Conclusion
