---
title: "DATA 622 - Homework4 - Mental Health Analysis"
author: "Group1: Diego Correa, Amanda Arce, Soumya Ghosh & Atina Karim"
date: "November 08, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(rpart.plot)
library(naniar)
library(xgboost)
library(DiagrammeR)
library(factoextra)
library(e1071)
library(FactoMineR)
```

## Background

For this assignment, we will be working with a very interesting mental health dataset from a real-life research project. All identifying information, of course, has been removed. The attached spreadsheet has the data (the tab name “Data”). The data dictionary is given in the second tab. You can get as creative as you want. The assignment is designed to really get you to think about how you could use different methods.

The target variable is **'Suicide Attempt'**.

### Data Dictionary

![**ADHD Data Dictionary**](https://github.com/dcorrea614/DATA622/blob/main/HW4/Images/Data%20Dictionary%20Pic.PNG)

```{r echo=FALSE, results='asis'}
cat(
  '![](https://github.com/dcorrea614/DATA622/blob/main/HW4/Images/Data%20Dictionary%20Pic.PNG',
  if (knitr::is_html_output()) '?raw=true',
  '){width=700px}',
  sep = ''
)
```

### Problem Statement

1. Conduct a thorough Exploratory Data Analysis (EDA) to understand the dataset. (20 points)
2. Use a clustering method to find clusters of patients here. Whether you choose to use k-means
clustering or hierarchical clustering is up to you as long as you reason through your work. You
are free to be creative in terms of which variables or some combination of those you want to use. Can you come up with creative names for the profiles you found? (40 points)
3. Let’s explore using Principal Component Analysis on this dataset. You will note that there are different types of questions in the dataset: column: E-W: ADHD self-report; column X – AM:
mood disorders questionnaire, column AN-AS: Individual Substance Misuse; etc. You could just
use ONE of the sets of questionnaire, for example, you can conduct PCA on the ADHD score, or
mood disorder score, etc. Please reason through your work as you decide on which sets of
variables you want to use to conduct Principal Component Analysis. What did you learn from the
PCA? Can you comment on which question may have a heavy bearing on the score? (40 points)
4. Assume you are modeling whether a patient attempted suicide (column AX). This is a binary
target variable. Please use Gradient Boosting to predict whether a patient attempts suicides.
Please use whatever boosting approach you deem appropriate. But please be sure to walk us
through your steps. (50 points)
5. Using the same target variable (suicide attempt), please use support vector machine to model this. You might want to consider reducing the number of variables or somehow use extracted
information from the variables. This can be a really fun modeling task! (50 points)


## Dataset

```{r warning=FALSE, message=FALSE}
dataset <- read_csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/HW4/ADHD_data.csv')
head(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")
```

### Descriptive Dataset Summary

```{r warning=FALSE, message=FALSE}
summary(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


## Pre-Processing

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r fig.height=4, message=FALSE, warning=FALSE}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(dataset, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(dataset, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts <- dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`))

dataset_missing_counts  %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  geom_label(aes(label = NA_Count)) +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())

```


```{r}
# Use nanair package to plot missing value patterns
gg_miss_upset(dataset)
```

### Data Imputation 

Based on above missing value analysis, we are going to perform data imputation using the  **mice** package following Random Forest method. But before that, we remove the Initial as we do not need the ID columns for imputation.  Additionally, we removed the ADHD and MDQ subtotal columns to avoid collinearity.

```{r}
# removing Initial, ADHD Total and MD Total columns
dataset <- dataset %>%
  select(-c('Initial','ADHD Total','MD TOTAL'))

# cleaning up the column names for the imputation function
colNamesNoSpace <- colnames(dataset) %>%
  str_remove_all(' |-|\\.')

colnames(dataset) <- colNamesNoSpace
```


```{r message=FALSE, warning=FALSE}
#imputation by using the random forest method ('rf')
init <- mice(dataset, maxit = 0)
predM <- init$predictorMatrix
set.seed(123)
imputed <- mice(dataset, method = 'rf', predictorMatrix = predM, m=5)
```

```{r}
dataset <- complete(imputed)
summary(dataset)
```

We also checked for presence of any de-generate variables and found no such variable present in our dataset  

```{r}
# capturing the degenerate variables
degenCols <- nearZeroVar(dataset)

# identifying them 
colnames(dataset[,degenCols])

# removing from the dataset
dataset <- dataset[,-degenCols]
```

## Exploratory Data Analysis

```{r fig.height= 8, fig.width=8}
# make dataset long to place distribution in a facetwrap
vars <- dataset %>%
  gather(key = 'predictor_variable', value = 'value', -Suicide) %>%
  mutate(Suicide = ifelse(Suicide==1,'Y','N'))

# Distribution of ADHD variables
vars %>%
  filter(str_detect(predictor_variable,'ADHD')) %>%
  ggplot() +
  geom_histogram(aes(x = value, y = ..density.., fill = Suicide), bins = 15) +
  labs(title = 'Distributions of ADHD Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)


# Distribution of MDQ variables
vars %>%
  filter(str_detect(predictor_variable,'MD')) %>%
  ggplot() +
  geom_histogram(aes(x = value, y = ..density.., fill = Suicide), bins = 15) +
  labs(title = 'Distributions of MDQ Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)

# Distribution of other variables
vars %>%
  filter(!str_detect(predictor_variable,'MD|ADH')) %>%
  ggplot() +
  geom_histogram(aes(x = value, y = ..density.., fill = Suicide), bins = 15) +
  labs(title = 'Distributions of Other Variables') +
  theme(plot.title = element_text(hjust = 0.5)) +
  facet_wrap(. ~predictor_variable, scales = 'free', ncol = 3)
```



### Correlation Plot: Multicollinearity Check


```{r fig.height= 8, fig.width=8, message=FALSE, warning=FALSE}
corrMatrix <- round(cor(dataset),4)
corrMatrix %>% corrplot(., method = "color", outline = T, addgrid.col = "darkgray", order="hclust", addrect = 4, rect.col = "black", rect.lwd = 5,cl.pos = "b", tl.col = "indianred4", tl.cex = 1.0, cl.cex = 1.0, addCoef.col = "white", number.digits = 2, number.cex = 0.8, col = colorRampPalette(c("darkred","white","dodgerblue4"))(100))
```


### Splitting Data: Train/Test

We are going to do a 75-25% split for training and test purposes. 

```{r}
sample = sample.split(dataset$Suicide, SplitRatio = 0.75)
train = subset(dataset, sample == TRUE)
test = subset(dataset, sample == FALSE)

#head(train)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")


```


## Model Building



#Creating seperate dataframe for 'Loan_Status' feature which is our target.
```{r}
y_train <- as.factor(train$Suicide)
y_test <- as.factor(test$Suicide)

X_train <- train %>% select(-'Suicide')
X_test <- test %>% select(-'Suicide')
```


### Problem2: Clustering Method

K-means clustering is an example of an unsupervised machine learning algorithm.  A clustering algorithm is a collection of data points that are aggregated together because of certain similarities.  The "k" in k-means is the number of centoids you need in the dataset - or "imaginary or real location represeting the center of the cluster".  The "means" in k-means refers to averaging of the data, or finding the centroid. 


```{r}
#select numeric values only
nums <- X_test %>% dplyr::select(where(is.numeric))
#center and scale data for K-means algorithm
dftrans <- preProcess(nums, method = c("center", "scale", "BoxCox"))
dftrans <- predict(dftrans, nums)

```


```{r}
library(factoextra)
#determine # of clusters
fviz_nbclust(dftrans, kmeans, method = "silhouette", k.max = 10)

```



```{r}
library(cluster)
#generate cluster plot
d <- dist(t(dftrans), method="euclidian")   
kfit <- kmeans(d, 2)   
clusplot(as.matrix(d), kfit$cluster, color=T, shade=T, labels=2, lines=0) 
```

#### Model Summary

```{r}
predict_km <- kmeans(dftrans, 2, nstart = 25)
print(predict_km)
```


```{r}
dftrans %>%
  mutate(Cluster = predict_km$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean") %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")
  
```



### Problem3: Principal Component Analysis (PCA)

Principal Component Analysis (PCA) is an unsupervised, non-parametric statistical technique primarily used for dimensionality reduction in machine learning. It is a useful technique for exploratory data analysis, allowing us to better visualize the variation present in a dataset with many variables. For our particular use case here, it appears that many of the questionnaire variables fall on likert scales, which when prepared for analysis are extended to dummy variables. This creates many additional features and can make analysis more difficult due to an increased number of dimensions. Therefore, utilizing PCA to reduce the number of dimensions on our entired dataset and measure the amount of variance explained is beneficial. In order to do this, we’ll use the **prcomp()** function:

```{r}
dataset.pca <- prcomp(dataset, center = TRUE, scale = TRUE)

summary(dataset.pca)
```


The scale = 0 argument to biplot ensures that the arrows are scaled to represent the loadings; other values for scale give slightly different biplots with different interpretations.

```{r fig.height=7, fig.width=7}
biplot(dataset.pca, scale = 0, cex=0.5)
```

From the biplot above, it’s difficult to tell much given the very large number of features. However, from our PCA analysis, we can also take a look at the eigenvalues that were generated by using a scree plot. The plot below shows the percentage of variance explained by each principal component.

```{r}
fviz_eig(dataset.pca)
```

```{r}
#compute standard deviation of each principal component
std_dev <- dataset.pca$sdev

#compute variance
pr_var <- std_dev^2


prop_varex <- pr_var/sum(pr_var)

round(prop_varex[1:10], 2)
```

The first principal component in our example therefore explains 25% of the variability, and the second principal component explains 9%. Together, the first ten principal components explain 62% of the variability. And we proceed to plot the PVE and cumulative PVE to provide us our scree plots as we did earlier.

```{r}
#scree plot
plot(prop_varex, xlab = "Principal Component",
             ylab = "Proportion of Variance Explained",
             type = "b")
```

```{r}
#cumulative scree plot
plot(cumsum(prop_varex), xlab = "Principal Component",
              ylab = "Cumulative Proportion of Variance Explained",
              type = "b")
```

As we can see above in our plots of the PVE and the cumultative PVE, the first few principal components account for a much larger proportion of the variance explained than the remaining 50+ principal components for this dataset. Additionally, the proportion of variance explained by the first principal component at 25% is ~3 times the second principal component’s proportion of variance explained.

Although running PCA on the entire dataset is helpful in some ways, certain findings from a biplot are affected by the number of variables in the overall dataset. Additionally, we are seeing from our PCA analysis that the first few principal components explain a much larger proportion of the variability than later principal components. Because of this, we thought it would be interesting to dive a bit deeper into which variables seem to hold the most importance in determining the first few dimensions. To do this, we’ll use the FactoMineR and factoextra packages to determine the eigenvalues of each dimension, with particular interest in dimensions 1 and 2, since those are what we’ve plotted above and hold the highest proportion of variance explained.

```{r}
eig.val <- get_eigenvalue(dataset.pca)
eig.val %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")
```


```{r}
res.pca <- PCA(dataset, scale.unit = TRUE, ncp = 5, graph = FALSE)
var <- get_pca_var(res.pca)
```

Interestingly, eigenvalues less than 1 indicate that principal components account for more variance than accounted by one of the original variables in the standardized data. Because of this, many use this as a cutoff point for which PCs should be retained. Since we see this cutoff occur at Dim = 50, anything past this dimension doesn’t provide good insight into our data. Above, our scree plot cut this even further, by showing that anything past the first ten dimensions does not account for a large proportion of variance explained. Therefore, going forward, we will only focus on the first few dimensions to see if we can garner any insights.

Since our biplot above was very crowded and difficult to interpret, we decided to look a bit deeper at the quality of representation of the variables, which is determined by taking the square cosine (cos2) and accounts for a variable’s position relative to the circumference of the correlation circle (not pictured in our biplot above but can be visually seen by the length of each vector from the center/origin). After subjecting these cos2 values to a corrplot across the first five PCs, we can examine below:

```{r fig.height= 8, fig.width=8}
corrplot(var$cos2, is.corr=FALSE)
```

Typically, a high cos2 value indicates a good representation of the variable on the principal component, and in our case the variable is positioned close to the circumference of the correlation circle (and farther away from the origin) – which we can visibly see with variables such as X.ADHD.Total and X.MD.Total on the biplot. The opposite is true for variables with a low cos2 value, which tend to fall closer to the origin.

We can also take a look at factors such as the contribution of variables on our principal components. Variables that are correlated with PC1 and PC2 are the most important in explaining the variability in the dataset. Therefore, when we conduct another correlation plot, but this time of variable contribution, we see the following:

```{r fig.height= 8, fig.width=8}
corrplot(var$contrib, is.corr=FALSE)
```

We can also see this in a barplot below, and shows the variables that have the highest contribution percentage for our first two PCs:

```{r}
fviz_contrib(dataset.pca, choice = "var", axes = 1, top = 15)
fviz_contrib(dataset.pca, choice = "var", axes = 2, top = 15)
```

Similar to our biplot visualization and our cos2 values, we can confirm that ADHDQ8, ADHDQ9 etc. along with many other ADHD and MD questions contribute most to the variability explained in our dataset. This is important for us to take note of for future analysis, where we will be looking more closely at features that seem to provide more insight into clustering and classification.

Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.

```{r}
fviz_pca_var(dataset.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
```

#### Model Summary


### Problem4: Gradient Boosting Method
The boosting method is uses trees in a sequential pattern.  The successive tree is developed using information from the previous tree to minimize its error.

The boosting method has three tuning parameters, number of trees, shrinkage parameter, and number of splits in a tree.

We will be using the stochastic gradient boosting in our model.  This approach resamples the observations and columns in each iteration.  

The xgbTree method will create a model by choosing the best tuning parameters.

```{r include=FALSE}
set.seed(312)
gbmTune <- train(X_train, y_train,
                 method = 'xgbTree',
                 trControl = trainControl('cv',number = 10),
                 verbose = FALSE)
```

Here, we can see our parameters that are used in the model.

```{r}
gbmTune$bestTune
```
We identify the variables with the most importance to the prediction below, with Abuse and MDQ1g being highly important.  

```{r}
varImp(gbmTune)
```
Visualization of the first 3 trees used in the model.

```{r}
xgb.plot.tree(model = gbmTune$finalModel,trees = 1:3)
```

Now, to look at how the model performed on the test dataset.

```{r}
gbm_predict <- predict(gbmTune, newdata = X_test)
gbm_conf_matrix <- confusionMatrix(gbm_predict, y_test)
print(gbm_conf_matrix)
```


#### Model Summary

We record the summary of the Stochastic gradient boosting model metrics in a data frame -

```{r}
gbm_model <- confusionMatrix(table(gbm_predict, y_test))$byClass
gbm_accuracy <- confusionMatrix(table(gbm_predict, y_test))$overall['Accuracy']
gbm_model <- data.frame(gbm_model)
gbm_model <- rbind("Accuracy" = gbm_accuracy, gbm_model)
```


### Problem5: Support Vector Machine Model

Support Vector Machines are supervised learning models used to analyze data for classification problems. 

For our dataset, SVM will help us decide an optimal decision boundary which can then help classify suicide attempts. We will build a linear model as well as a radial model and evaluate the performance for each to determine which SVM model is appropriate given our data.  

#### Feature Selection

We would like to calculate the correlation between our independent variables and the target, Suicide. This is to identify the most significant predictors of suicide attempts. 

```{r}
#Compute correlation between each variable and Suicide
target_corr <- function(x, y) cor(y, x)
Suicide_Correlation <- sapply(train, target_corr, y=train$Suicide) 

#Output Correlation with Target
Suicidecorr <- data.frame(Suicide_Correlation)
Suicidecorr %>%
  kbl(caption = "Correlation with Suicide") %>%
  kable_minimal()
```
There seems to be no strong correlation present in the data. Out of the independent variables,history of abuse alcohol usage maybe indicators of suicide attempts among the respondents. Certain ADHD and MD responses also seem to have a moderate correlation with suicide.

We will keep the variables that had a somewhat strong positive correlation such as abuse, alcohol, MDQ1G,MDQ1D and ADHDQ1. For the rest, we will use caret package to determine which features maybe significant in predicting suicide attempts.

```{r}
set.seed(7)
# prepare training scheme
control <- trainControl(method="repeatedcv", number=10, repeats=3)
# train the model
model <- train(as.factor(Suicide)~., data=dataset, method="lvq", preProcess="scale", trControl=control)
# estimate variable importance
importance <- varImp(model, scale=FALSE)
# summarize importance
print(importance)
```
We had already identified some of the variables above from the correlation matrix. We will keep the top 10 important variables for our SVM model.

##### Linear SVM

First, let's build the linear model with all variables (including those deemed insignificant). We will set the cost parameter to 10.

```{r}
linear_svm1=svm(formula=as.factor(Suicide)~.,data=train, kernel="linear", cost=10, scale=FALSE)
print(linear_svm1)
```
```{r}
test$pred = predict(linear_svm1, test)
 
confusionMatrix(test$pred,as.factor(test$Suicide))
```
Let's see if including only the important features improves model fit.

```{r}
linear_svm2=svm(formula=as.factor(Suicide)~ Abuse+MDQ1g+Alcohol+MDQ1d+MDQ1b+MDQ2+ADHDQ1+MDQ3+Opioids+MDQ1a,data=train, kernel="linear", cost=10, scale=FALSE)
print(linear_svm2)
```
```{r}
test$pred = predict(linear_svm2, test)
 
confusionMatrix(test$pred,as.factor(test$Suicide))
```
The model fit has significantly improved after removing the insignificant features. 


##### Radial SVM

First we will build a radial model with all variables
```{r}
radial_svm = svm(as.factor(Suicide)~., data=train, C = 13)
print(radial_svm)
```
```{r}
test$pred = predict(radial_svm, test)
 
confusionMatrix(test$pred,as.factor(test$Suicide))
```

We will build another radial model with only the most significant variables

```{r}
radial_svm2 = svm(as.factor(Suicide)~Abuse+MDQ1g+Alcohol+MDQ1d+MDQ1b+MDQ2+ADHDQ1+MDQ3+Opioids+MDQ1a, data=train, C = 13)
print(radial_svm2)
```
```{r}
test$pred = predict(radial_svm2, test)
 
confusionMatrix(test$pred,as.factor(test$Suicide))
```
he model fit for the radial model has improved after removing the insignificant variables - however, model performance is still better for the linear model.

#### Model Summary

We will store the results of our final SVM model in a dataframe

```{r}
SVM_Model_Final <- confusionMatrix(test$pred,as.factor(test$Suicide))$byClass
SVM_Model_Final <- data.frame(SVM_Model_Final)
AccuracySVM <- confusionMatrix(test$pred, as.factor(test$Suicide))$overall['Accuracy']
SVM_Model_Final<- rbind("Accuracy" = AccuracySVM, SVM_Model_Final)
tabularview <- data.frame(SVM_Model_Final)
tabularview %>%  kableExtra::kbl() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive"),latex_options="scale_down")

```


## Conclusion



