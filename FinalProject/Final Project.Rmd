---
title: "DATA 622 - Final Project - Communities and Crimes Analysis"
author: "Group1: Diego Correa, Amanda Arce, Soumya Ghosh & Atina Karim"
date: "December 02, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(rpart.plot)
library(naniar)
library(xgboost)
library(DiagrammeR)
```


## Background

For this assignment, we will be working with a very interesting mental health dataset from a real-life research project. All identifying information, of course, has been removed. The attached spreadsheet has the data (the tab name “Data”). The data dictionary is given in the second tab. You can get as creative as you want. The assignment is designed to really get you to think about how you could use different methods.

The target variable is **'Violent Crimes Per Population'**.

## Data Dictionary


## Problem Statement


## Dataset

```{r warning=FALSE, message=FALSE}
dataset <- read_csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/FinalProject/communities.data.csv')
head(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")
```


### Descriptive Dataset Summary

We see that *?* is used in place of NA in the dataset.  We need to convert these entries into *NA* values to begin our analysis.

```{r warning=FALSE, message=FALSE}
# 
dataset[dataset == '?'] = NA

summary(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

## Pre-Processing

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r fig.height=4, message=FALSE, warning=FALSE}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(dataset, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(dataset, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts <- dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`))

dataset_missing_counts  %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  geom_label(aes(label = NA_Count)) +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())
```
There are 22 variables missing 84% of data and 2 variables missing roughly 59% of data.  Imputation on these varibales is a risk as the majority of the data is missing.  Instead we can remove these variables from our dataset. 

```{r}
colremove <- dataset_missing_counts %>%
  filter(NA_Percentage > 50) %>% 
  select(Feature) %>%
  as.list()

dataset <- dataset[,!(colnames(dataset) %in% colremove$Feature)]


# removing the folds variable as it was placed for cross validation
dataset <- dataset[,names(dataset) != 'fold']
```

```{r}
# capturing the degenerate variables
degenCols <- nearZeroVar(dataset)

# identifying them 
colnames(dataset[,degenCols])

# removing from the dataset
dataset <- dataset[,-degenCols]

dataset <- dataset[,!colnames(dataset) == 'communityname']

dataset <- dataset %>%
  mutate(OtherPerCap = as.numeric(OtherPerCap))

#remove missing brand => df3
dataset <- dataset[!is.na(dataset$OtherPerCap), ]
```

## Exploratory Data Analysis

### Graphing on Map

```{r}
# aggregating the Average ViolentCrimesPerPop by State ID (FIPS)
avgViolentCrimeByState <- dataset %>%
  group_by(state) %>%
  summarise(avgViolentCrimesPerPop = mean(ViolentCrimesPerPop)) %>%
  mutate(fips = str_pad(state,2,pad='0'))

plot_usmap(data = avgViolentCrimeByState, values = 'avgViolentCrimesPerPop', color = 'red') +
  scale_fill_continuous(
    low = 'white',high = 'red',name = 'Average Violent Crimes per 100K Population'
  ) + theme(legend.position = 'right')
```



### Distributions

### Correlation Plot: Multicollinearity Check




### Splitting Data: Train/Test

We are going to do a 75-25% split for training and test purposes. 

```{r}
sample = sample.split(dataset$ViolentCrimesPerPop, SplitRatio = 0.75)


train = subset(dataset, sample == TRUE) %>% as.matrix()
test = subset(dataset, sample == FALSE) %>% as.matrix()

y_train <- train[,101]
y_test <- test[,101]

X_train <- train[,-101]
X_test <- test[,-101]

#head(train)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

```


## Modeling

### Linear Regression Model 

### Non Linear Regression Model

Non Linear Regression models do not follow the $y=ax+b$ approach.  It allows for flexibility in the model to fit non linear trends in data.  We will go over three examples of non linear regression: K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multivaraite Adaptive Regression Splines (MARS).

#### K-Nearest Neighbors

The **KNN** model predicts a new sample for regression using the **K**-closest data points from the training set.  The method uses the Euclidean distance or the straight-line distance between samples.


$$(\sum^p_{j=1}(x_{aj}-x_{bj})^q)^{1/q}$$

The `train` function in the **caret** package uses the `knn` method, centers and scales 

```{r}
knnFit <- train(X_train, y_train,
                   method = 'knn',
                   preProc = c('center','scale'),
                   tuneLength = 14,
                   trControl = trainControl(method = 'cv'))

knnFit$finalModel

knnFit
```

##### KNN Plot

```{r}
plot(knnFit)
```

###### Variable Importance

```{r}
varimp <- varImp(knnFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


###### Prediction Performance

```{r}
knnPred <- predict(knnFit, newdata = X_test)
knnPredPerf <- postResample(pred = knnPred, obs = y_test)

knnPredPerf['Family'] <- 'NonLinear'
```


#### Suppost Vector Machines

SVM regression models try to find a regression line where each data point is within a threshold.  The error is a parameter of the model and it will then try to minimize the coefficients used in the model. This allows the model eliminate features and have greater interpretability.

We will again use the **caret** package and apply a **radial SVM** model.  This method will allow the model to find trends in the dataset.


```{r}
svmRFit <- train(X_train, y_train,
                   method = 'svmRadial',
                   preProc = c('center','scale'),
                   tuneLength = 14,
                   trControl = trainControl(method = 'cv'))

svmRFit
```

###### Variable Importance

```{r}
varimp <- varImp(svmRFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


###### Prediction Performance

```{r}
svmRPred <- predict(svmRFit, newdata = X_test)
svmRPredPerf <- postResample(pred = svmRPred, obs = y_test)

svmRPredPerf['Family'] <- 'NonLinear'
```

#### Multivaraite Adaptive Regression Splines

The Mars model finds *knots* to determine piece wise linear model where each new feature models an isolated portion of the data.  The predictor / cut point combination that has the smallest error is used for the model in an iterative process until a stopping point is reached.  


We will use the **earth** package to generate this MARS model.


```{r}
# mars
marsFit <- earth(X_train, y_train)
summary(marsFit)
```

##### Visualizing MARS Model

```{r}
plotmo(marsFit)
```

###### Variable Importance

```{r}
varimp <- varImp(marsFit)

varimp %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


###### Prediction Performance

```{r}
marsPred <- predict(marsFit, newdata = X_test)
marsPredPerf <- postResample(pred = marsPred, obs = y_test)

marsPredPerf['Family'] <- 'NonLinear'
```

### Trees and Boosting Model

## Conclusion
