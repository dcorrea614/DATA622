---
title: "DATA 622 - Final Project - Communities and Crimes Analysis"
author: "Group1: Diego Correa, Amanda Arce, Soumya Ghosh & Atina Karim"
date: "December 02, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(rpart.plot)
library(naniar)
library(xgboost)
library(DiagrammeR)
```


## Background

For this assignment, we will be working with a very interesting mental health dataset from a real-life research project. All identifying information, of course, has been removed. The attached spreadsheet has the data (the tab name “Data”). The data dictionary is given in the second tab. You can get as creative as you want. The assignment is designed to really get you to think about how you could use different methods.

The target variable is **'Violent Crimes Per Population'**.

## Data Dictionary


## Problem Statement


## Dataset

```{r warning=FALSE, message=FALSE}
dataset <- read_csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/FinalProject/communities.data.csv')
head(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")
```


### Descriptive Dataset Summary

We see that *?* is used in place of NA in the dataset.  We need to convert these entries into *NA* values to begin our analysis.

```{r warning=FALSE, message=FALSE}
# 
dataset[dataset == '?'] = NA

summary(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

## Pre-Processing

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r fig.height=4, message=FALSE, warning=FALSE}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(dataset, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(dataset, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts <- dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`))

dataset_missing_counts  %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  geom_label(aes(label = NA_Count)) +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())
```
There are 22 variables missing 84% of data and 2 variables missing roughly 59% of data.  Imputation on these varibales is a risk as the majority of the data is missing.  Instead we can remove these variables from our dataset. 

```{r}
colremove <- dataset_missing_counts %>%
  filter(NA_Percentage > 50) %>% 
  select(Feature) %>%
  as.list()

dataset <- dataset[,!(colnames(dataset) %in% colremove$Feature)]


# removing the folds variable as it was placed for cross validation
dataset <- dataset[,names(dataset) != 'fold']
```

```{r}
# capturing the degenerate variables
degenCols <- nearZeroVar(dataset)

# identifying them 
colnames(dataset[,degenCols])

# removing from the dataset
dataset <- dataset[,-degenCols]

dataset <- dataset[,!colnames(dataset) == 'communityname']

dataset <- dataset %>%
  mutate(OtherPerCap = as.numeric(OtherPerCap))

#remove missing brand => df3
dataset <- dataset[!is.na(dataset$OtherPerCap), ]
```

## Exploratory Data Analysis

### Graphing on Map

```{r}
# aggregating the Average ViolentCrimesPerPop by State ID (FIPS)
avgViolentCrimeByState <- dataset %>%
  group_by(state) %>%
  summarise(avgViolentCrimesPerPop = mean(ViolentCrimesPerPop)) %>%
  mutate(fips = str_pad(state,2,pad='0'))

plot_usmap(data = avgViolentCrimeByState, values = 'avgViolentCrimesPerPop', color = 'red') +
  scale_fill_continuous(
    low = 'white',high = 'red',name = 'Average Violent Crimes per 100K Population'
  ) + theme(legend.position = 'right')
```



### Distributions

### Correlation Plot: Multicollinearity Check




### Splitting Data: Train/Test

We are going to do a 75-25% split for training and test purposes. 

```{r}
sample = sample.split(dataset$ViolentCrimesPerPop, SplitRatio = 0.75)


train = subset(dataset, sample == TRUE) %>% as.matrix()
test = subset(dataset, sample == FALSE) %>% as.matrix()

y_train <- train[,101]
y_test <- test[,101]

X_train <- train[,-101]
X_test <- test[,-101]

#head(train)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

```


## Modeling


### Linear Regression Model - Ridge and LASSO



The glmnet package allows us to perform both the ridge and lasso regression models.  This library requires the response variable to be a vector and the predictor variables to be a class of data.matrix.
```{r}
library(glmnet)
```


#Ridge Regression

Ridge regression is a model used "when the number of predictor variables in a set exceeds the number of observations", or when a dataset suffers from multicollinearity.  According to stats, with ridge regression, often predictor variables used in a regression are highly correlated.  When they are, the regression coefficient of any one variable can depend on which other predictor variables are included in the model, and which ones are left out. 


Now we'll use the glmnet() function to fit the ridge regression model and specify alpha=0.
```{r}

fit.ridge <- cv.glmnet(as.matrix(X_train), as.matrix(y_train), alpha = 0, type.measure = "mse", family="gaussian")
````

To identify what value to use for lambda, we could plot the function using our model fit.ridge$lambda.min, but below we'll use the s="lambda.min" argument to save time.  The graph will be shown below after all models.

```{r}

fitted.ridge.train <- predict(fit.ridge, newx = data.matrix(X_train), s="lambda.min")


fitted.ridge.test <- predict(fit.ridge, newx = data.matrix(X_test), s="lambda.min")

cat("Train coefficient: ", cor(as.matrix(y_train), fitted.ridge.train)[1])
cat("\nTest coefficient: ", cor(as.matrix(X_test), fitted.ridge.test)[1])
```
As we can see above, the train and test coefficients are 0.77 and -0.364




#LASSO Regression - Least Absolute Shrinkage and Selection Operator

According to statisticshowto.com, lasso regression is a type of linear regression that uses shrinkage - which shrinks datapoints towards a central point, such as the mean. 



Now we'll use the glmnet() function to fit the LASSO regression model and specify alpha=1.  We'll specific s="lambda.min" to find the best lambda. 

```{r}
fit.lasso <- cv.glmnet(as.matrix(X_train), as.matrix(y_train), type.measure="mse", alpha=1, family="gaussian")


fitted.lasso.train <- predict(fit.lasso, newx = data.matrix(X_train), s="lambda.min")

fitted.lasso.test <- predict(fit.lasso, newx = data.matrix(X_test), s="lambda.min")

cat("Train coefficient: ", cor(as.matrix(y_train), fitted.lasso.train)[1])
cat("\nTest coefficient: ", cor(as.matrix(X_test), fitted.lasso.test)[1])

```
As we can see above, the train and test coefficients are 0.83 and -0.24.


#Elastic Net Regression

According to machinelearningmastery.com, Elastic Net is an extension of linear regression that will add regulirization penalties to the loss function during training. 

```{r}
fit.elnet <- glmnet(as.matrix(X_train), as.matrix(y_train), family="gaussian", alpha=.5)

fit.elnet.cv <- cv.glmnet(as.matrix(X_train), as.matrix(y_train), type.measure="mse", alpha=.5,
                          family="gaussian")

fitted.elnet.train <- predict(fit.elnet.cv, newx = data.matrix(X_train), s="lambda.min")

fitted.elnet.test <- predict(fit.elnet.cv, newx = data.matrix(X_test), s="lambda.min")

cat("Train coefficient: ", cor(as.matrix(y_train), fitted.elnet.train)[1])
cat("\nTest coefficient: ", cor(as.matrix(X_test), fitted.elnet.test)[1])

```


Plot MSE
```{r message=FALSE, warning=FALSE}
par(mfrow=c(3,1))

plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
plot(fit.elnet.cv, xvar="lambda")

```











### Non Linear Regression Model

Non Linear Regression models do not follow the $y=ax+b$ approach.  It allows for flexibility in the model to fit non linear trends in data.  

We will go over three examples of non linear regression: K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multivaraite Adaptive Regression Splines (MARS).


#### Neural Networks

Neural Networks are regression techniques inspired by theories of how the brain works (neurons signalling to one another).  The outcome is modeled by an input layer and an intermediary set of unobserved values, hidden units, made up of linear combinations of the original predictors.  

The parameters used in the model include number of hidden units, decay, number of iterations to find parameter estimates, and the number of paramters used by the model.


```{r}
# averaging a simple Neural Network with 5 hidden units
nnetAvg <- avNNet(X_train, y_train,
                  size = 5,
                  decay = 0.01,
                  repeats = 5,
                  linout = TRUE,
                  trace = FALSE,
                  maxit = 500,
                  MaxNWts = 5 * (ncol(X_train) + 1) +5+1)

nnetAvg
```


###### Variable Importance

```{r}
varimp <- varImp(nnetAvg)

varimp %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

###### Prediction Performance

```{r}
nnetPred <- predict(nnetAvg, newdata = X_test)
nnetPredPerf <- postResample(pred = nnetPred, obs = y_test)

nnetPredPerf['Family'] <- 'NonLinear'
```

#### K-Nearest Neighbors

The **KNN** model predicts a new sample for regression using the **K**-closest data points from the training set.  The method uses the Euclidean distance or the straight-line distance between samples.


$$(\sum^p_{j=1}(x_{aj}-x_{bj})^q)^{1/q}$$

The `train` function in the **caret** package uses the `knn` method, centers and scales 

```{r}
knnFit <- train(X_train, y_train,
                   method = 'knn',
                   preProc = c('center','scale'),
                   tuneLength = 14,
                   trControl = trainControl(method = 'cv'))

knnFit$finalModel

knnFit
```

##### KNN Plot

```{r}
plot(knnFit)
```

###### Variable Importance

```{r}
varimp <- varImp(knnFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

###### Prediction Performance

```{r}
knnPred <- predict(knnFit, newdata = X_test)
knnPredPerf <- postResample(pred = knnPred, obs = y_test)

knnPredPerf['Family'] <- 'NonLinear'
```

#### Suppost Vector Machines

SVM regression models try to find a regression line where each data point is within a threshold.  The error is a parameter of the model and it will then try to minimize the coefficients used in the model. This allows the model eliminate features and have greater interpretability.

We will again use the **caret** package and apply a **radial SVM** model.  This method will allow the model to find trends in the dataset.


```{r}
svmRFit <- train(X_train, y_train,
                   method = 'svmRadial',
                   preProc = c('center','scale'),
                   tuneLength = 14,
                   trControl = trainControl(method = 'cv'))

svmRFit
```

###### Variable Importance

```{r}
varimp <- varImp(svmRFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

###### Prediction Performance

```{r}
svmRPred <- predict(svmRFit, newdata = X_test)
svmRPredPerf <- postResample(pred = svmRPred, obs = y_test)

svmRPredPerf['Family'] <- 'NonLinear'
```

#### Multivaraite Adaptive Regression Splines

The Mars model finds *knots* to determine piece wise linear model where each new feature models an isolated portion of the data.  The predictor / cut point combination that has the smallest error is used for the model in an iterative process until a stopping point is reached.  


We will use the **earth** package to generate this MARS model.


```{r}
# mars
marsFit <- earth(X_train, y_train)
summary(marsFit)
```

##### Visualizing MARS Model

```{r}
plotmo(marsFit)
```

###### Variable Importance

```{r}
varimp <- varImp(marsFit)

varimp %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


###### Prediction Performance

```{r}
marsPred <- predict(marsFit, newdata = X_test)
marsPredPerf <- postResample(pred = marsPred, obs = y_test)

marsPredPerf['Family'] <- 'NonLinear'
```

### Trees and Boosting Model

## Conclusion
