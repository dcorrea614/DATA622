---
title: "DATA 622 - Final Project - Communities and Crimes Analysis"
author: "Group1: Diego Correa, Amanda Arce, Soumya Ghosh & Atina Karim"
date: "December 02, 2021"
always_allow_html: yes
output:
  html_document:
    df_print: kable
    theme: cerulean
    highlight: pygments
    css: ./lab.css
    toc: true
    toc_float:
      collapsed: true
    toc_depth: 5
  pdf_document: 
    number_sections: yes
    toc: yes
    toc_depth: 5
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

```{r warning=FALSE, message=FALSE}
library(kableExtra)
library(tidyverse)
library(ggplot2)
library(dplyr)
library(psych)
library(caret)
library(mice)
library(randomForest)
library(caTools)
library(corrplot)
library(class)
library(rpart)
library(rpart.plot)
library(naniar)
library(xgboost)
library(usmap)
library(DiagrammeR)
library(earth)
library(plotly)
library(wordcloud)
library(RColorBrewer)
library(glmnet)
```


## Background

For the final project, we will be working with the **Communities & Crime** dataset from communities within United States. The data combines socio-economic data from the 1990 US Census, law enforcement data from the 1990 US LEMAS survey, and crime data from the 1995 FBI UCR. 

## Problem Statement

Every major violent crime that makes it to the news brings new notions and beliefs that tend to alter the opinion of the public in terms of the safety of their neighborhood and the socio-economic factors that influence the normal functioning of society. For example, with the increase in the number of race-related crimes in our society, it has become imperative to understand whether the racial profile has anything to do with crime rates in a region. Other concerns besides race, include socio-economic status, religious beliefs and the percentage of the population that is composed of immigrants.

Understanding where violent crimes occur in terms of the socio-economic, environmental, and demographic characteristics of the reported regions can be crucial to deciphering why these crimes occur. These characteristics may very well help in predicting in advance where violent crimes are likely to occur through predictive models that can quantify the risk associated with a region. This would greatly help in city planning through the deployment of police forces effectively in order to quell the imminent danger. Determining the key drivers that contribute to the rise in violent crimes will provide invaluable inputs in terms of urban development and design of policing practices.

## Dataset Overview

The ‘Communities and Crime’ dataset made available by the University of California, Irvine’s Machine Learning Repository provides an excellent opportunity to test some of these pre-conceived notions prevalent in our society today with regard to race and crimes. It could also be helpful in building predictive models that can help better in city planning and crime reduction. Although the sources in this dataset date back to 1990 and 1995, the directional insights we get from it should still hold true.

Many variables are included in the dataset so that algorithms that select or learn weights for attributes could be tested. However, clearly unrelated attributes were not included; attributes were picked if there was any plausible connection to crime (N=122), plus the attribute to be predicted (Per Capita Violent Crimes). The variables included in the dataset involve the community, such as the percent of the population considered urban, and the median family income, and involving law enforcement, such as per capita number of police officers, and percent of officers assigned to drug units.

The per capita violent crimes variable was calculated using population and the sum of crime variables considered violent crimes in the United States: murder, rape, robbery, and assault. There was apparently some controversy in some states concerning the counting of rapes. These resulted in missing values for rape, which resulted in incorrect values for per capita violent crime. These cities are not included in the dataset. Many of these omitted communities were from the midwestern USA.

Data is described below based on original values. All numeric data was normalized into the decimal range 0.00-1.00 using an Unsupervised, equal-interval binning method. Attributes retain their distribution and skew (hence for example the population attribute has a mean value of 0.06 because most communities are small). E.g. An attribute described as 'mean people per household' is actually the normalized (0-1) version of that value.

The normalization preserves rough ratios of values WITHIN an attribute (e.g. double the value for double the population within the available precision - except for extreme values (all values more than 3 SD above the mean are normalized to 1.00; all values more than 3 SD below the mean are nromalized to 0.00)).

However, the normalization does not preserve relationships between values BETWEEN attributes (e.g. it would not be meaningful to compare the value for whitePerCap with the value for blackPerCap for a community)

A limitation was that the LEMAS survey was of the police departments with at least 100 officers, plus a random sample of smaller departments. For our purposes, communities not found in both census and crime datasets were omitted. Many communities are missing LEMAS data.

The target variable is **'Violent Crimes Per Population'**.

### Dataset Repository

![UCI Machine Learning Repository Link] (https://archive.ics.uci.edu/ml/datasets/communities+and+crime)

### Data Dictionary

The crime dataset has **128 attributes**. Below is a brief description of all attributes along with data type details -

```{r warning=FALSE, message=FALSE}
datadict <- read_csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/FinalProject/data_dictionary.csv')

datadict %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="500px")

```

### Supplimentary Data

We also loaded the supplimentary state level dataset from UCI Machine Learning repository to enrich our crime dataset with state level information -

```{r}
states <- read.csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/FinalProject/states.csv')

names(states) <- c("state_code","state","stateName","stateENS")

states %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="80%",height="300px")

```

## Dataset Preview

```{r warning=FALSE, message=FALSE}
dataset <- read_csv('https://raw.githubusercontent.com/dcorrea614/DATA622/main/FinalProject/communities.data.csv')
head(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="100%",height="300px")

dim(dataset)
```

The dataset has 1994 observations and 128 variable. 

### Descriptive Dataset Summary

The dataset contains no duplicate records, but it contains a lot of missing values, represented by “?”. We need to convert these entries into *NA* values to begin our analysis.

The first 5 non-predictive variables represent the name of the community, the state in which it is located, the county code, the community code, and the fold number for non-random 10 fold cross validation. A summary of the remaining variables is below:

```{r warning=FALSE, message=FALSE}
# 
dataset[dataset == '?'] = NA

summary(dataset)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

## Pre-Processing

### Missing Value Analysis

Based on the above descriptive data summary, there are quite a few variables with missing values. So we conducted an analysis of all missing values in various attributes to identify proper imputation technique.

```{r fig.height=6, message=FALSE, warning=FALSE}
## Counts of missing data per feature
dataset_missing_counts <- data.frame(apply(dataset, 2, function(x) length(which(is.na(x)))))
dataset_missing_pct <- data.frame(apply(dataset, 2,function(x) {sum(is.na(x)) / length(x) * 100}))

dataset_missing_counts <- cbind(Feature = rownames(dataset_missing_counts), dataset_missing_counts, dataset_missing_pct)
colnames(dataset_missing_counts) <- c('Feature','NA_Count','NA_Percentage')
rownames(dataset_missing_counts) <- NULL

dataset_missing_counts <- dataset_missing_counts %>% filter(`NA_Count` != 0) %>% arrange(desc(`NA_Count`))

dataset_missing_counts  %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

ggplot(dataset_missing_counts, aes(x = NA_Count, y = reorder(Feature, NA_Count))) + 
  geom_bar(stat = 'identity', fill = 'steelblue') +
  geom_label(aes(label = NA_Count)) +
  labs(title = 'Missing Counts') +
  theme(plot.title = element_text(hjust = 0.5), axis.title.y = element_blank(), axis.title.x = element_blank())
```

There are 22 variables missing 84% of data and 2 variables missing roughly 59% of data. These variables with a high proportion of NA's (greater than 50%) were removed as these were deemed practically useless in prediction or data exploration. Imputation on these varibales is a risk as the majority of the data is missing. Most of these variables came from the 1990 Law Enforcement Management and Admin Stats survey (Lemas). 

Also, we are going to remove the 'fold' variable orginally added for cross validation.

```{r}
colremove <- dataset_missing_counts %>%
  filter(NA_Percentage > 50) %>% 
  select(Feature) %>%
  as.list()

dataset <- dataset[,!(colnames(dataset) %in% colremove$Feature)]


# removing the folds variable as it was placed for cross validation
dataset <- dataset[,names(dataset) != 'fold']

```

We also did check for presence of any degenerate variables in the dataset and removed them. 


```{r}
# capturing the degenerate variables
degenCols <- nearZeroVar(dataset)

# identifying them 
colnames(dataset[,degenCols])

# removing from the dataset
dataset <- dataset[,-degenCols]
```

We removed the 'communityname' from the dataset and removed rows with NA values in 'OtherPerCap' column -

```{r}
#dataset <- dataset[,!colnames(dataset) == 'communityname']

dataset <- dataset %>%
  mutate(OtherPerCap = as.numeric(OtherPerCap))

#remove missing brand => df3
dataset <- dataset[!is.na(dataset$OtherPerCap), ]

dim(dataset)
```


After removing these variables, our dataset reduced to 101 variables.

## Exploratory Data Analysis

To start with our visual exploratory data analysis, we joined the crime dataset with state level enrichment dataset -

```{r}
# Join with states dataframe
dataset1 <- left_join(dataset, states, 
              by = c("state" = "state_code"))

dataset1 <- rename(dataset1, state_abbr = state.y)
```

### Violent Crime Rate Analysis

District of Columbia leads in terms of violent crime rates in the United States. Los Angeles has the second highest crime rate after DC, followed by Louisiana, South Carolina, Maryland and Florida. DC, however, have much higher crime rates in comparison to the other states, which have roughly almost the same crime rate. However, New York and Florida have very high populations in comparison to the others.

```{r fig.height=6, fig.width=9}
# aggregating the Average ViolentCrimesPerPop by State ID (FIPS)
crime_rate_by_state <- dataset1 %>%
  group_by(state_abbr) %>%
  summarise(AvgCrimesRate = mean(ViolentCrimesPerPop)) %>% 
  mutate(rank = dense_rank(desc(AvgCrimesRate)))

crime_rate_by_state %>% filter(rank <=5) %>% arrange(rank) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="60%",height="250px")

#Violent Crime Rates by States
crime_rate_by_state$hover <- with(crime_rate_by_state, paste(state_abbr,'<br>', "Crime Rank:",rank))
l <- list(color = toRGB("white"), width = 2)
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('blue')
)

p <- crime_rate_by_state %>% plot_geo(locationmode = 'USA-states') %>%
  add_trace(
    z = ~AvgCrimesRate, text = ~hover, locations = ~state_abbr, 
    color = ~AvgCrimesRate, colors = 'Blues'
  ) %>%
 colorbar(title = "Crime Rate/100K Population") %>%
  layout(
    title = 'Violent Crime Rates in the United States',
    geo = g
  )

p

```


### Map of percentage of black population in the US

More than 65% of the population of DC belongs to the African-American community. Other states with high percentage of African-Americans are Mississippi, Georgia, Louisiana, and Delaware. These are some of the states with the highest violent crime rates. No wonder crimes bring about perceptions of race. We need to investigate this further.

```{r fig.height=6, fig.width=9}
#Percentage of African-American population by state
black_pop_by_state <- dataset1 %>%
  group_by(state_abbr) %>%
  summarise(AvgBlackPopRate = mean(racepctblack )) %>% 
  mutate(rank = dense_rank(desc(AvgBlackPopRate)))

black_pop_by_state %>% filter(rank <=5) %>% arrange(rank) %>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% 
  scroll_box(width="60%",height="250px")

black_pop_by_state$hover <- with(black_pop_by_state, paste(state_abbr,'<br>', "Black Population  Rank:",rank))
l <- list(color = toRGB("white"), width = 2)
g <- list(
  scope = 'usa',
  projection = list(type = 'albers usa'),
  showlakes = TRUE,
  lakecolor = toRGB('blue')
)

p <- black_pop_by_state %>% plot_geo(locationmode = 'USA-states') %>%
  add_trace(
    z = ~AvgBlackPopRate, text = ~hover, locations = ~state_abbr, 
    color = ~AvgBlackPopRate, colors = 'Greens'
  ) %>%
 colorbar(title = "%Black") %>%
  layout(
    title = 'Percentage of African-American Population',
    geo = g
  )
p
```


### Word Cloud of most dangerous communities

We conclude our initial Visual Data Exploration by taking a look at the communities with the highest violent crime rates. Interestingly, Camden City in New Jersey showed at the top on the list of most dangerous communities in the US in terms of violent crime rates. Other dangerous cities when it comes to violent crime rates are BatonRouge and Alexandria  in Louisiana, Baltimore, Atlanta, Anniston, Bessemer and Birmingham in Alabma, Atlantic City. 

```{r warning=FALSE, message=FALSE}
#Word Cloud of Communities with the highest crime rates

communityCrime <- dataset1 %>% select(communityname,state_abbr, population, ViolentCrimesPerPop) %>% 
  mutate(Place = paste(communityname,state_abbr, sep = ", ")) %>% 
  group_by(Place) %>% 
  summarise(AvgCrimesRate = mean(ViolentCrimesPerPop)) %>% 
  arrange(desc(AvgCrimesRate)) %>% 
  head(15)

par(mfrow = c(1,1))
par(mar = rep(0,4))
set.seed(1)
wordcloud(words = communityCrime$Place, freq = communityCrime$AvgCrimesRate, scale=c(3,0.01),
          random.order=FALSE,
          colors=brewer.pal(8, "Dark2"))
```



### Splitting Data: Train/Test

We are going to do a 75-25% split for training and test purposes. 

```{r}
dataset <- dataset[,!colnames(dataset) == 'communityname']
sample = sample.split(dataset$ViolentCrimesPerPop, SplitRatio = 0.75)


train = subset(dataset, sample == TRUE) %>% as.matrix()
test = subset(dataset, sample == FALSE) %>% as.matrix()

y_train <- train[,101]
y_test <- test[,101]

X_train <- train[,-101]
X_test <- test[,-101]

#head(train)%>% kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="300px")

```


## Modeling

### Model Group A: Linear Regression Model - Ridge and LASSO

The glmnet package allows us to perform both the ridge and lasso regression models.  This library requires the response variable to be a vector and the predictor variables to be a class of data.matrix.

#### Model1: Ridge Regression

Ridge regression is a model used "when the number of predictor variables in a set exceeds the number of observations", or when a dataset suffers from multicollinearity.  According to stats, with ridge regression, often predictor variables used in a regression are highly correlated.  When they are, the regression coefficient of any one variable can depend on which other predictor variables are included in the model, and which ones are left out. 


Now we'll use the glmnet() function to fit the ridge regression model and specify alpha=0.

```{r}

fit.ridge <- cv.glmnet(as.matrix(X_train), as.matrix(y_train), alpha = 0, type.measure = "mse", family="gaussian")
```

To identify what value to use for lambda, we could plot the function using our model fit.ridge$lambda.min, but below we'll use the s="lambda.min" argument to save time.  The graph will be shown below after all models.

```{r}

fitted.ridge.train <- predict(fit.ridge, newx = data.matrix(X_train), s="lambda.min")


fitted.ridge.test <- predict(fit.ridge, newx = data.matrix(X_test), s="lambda.min")

cat("Train coefficient: ", cor(as.matrix(y_train), fitted.ridge.train)[1])
cat("\nTest coefficient: ", cor(as.matrix(X_test), fitted.ridge.test)[1])
```

As we can see above, the train and test coefficients are 0.77 and -0.364


#### Model2: LASSO Regression - Least Absolute Shrinkage and Selection Operator

According to statisticshowto.com, lasso regression is a type of linear regression that uses shrinkage - which shrinks data points towards a central point, such as the mean. 

Now we'll use the glmnet() function to fit the LASSO regression model and specify alpha=1.  We'll specific s="lambda.min" to find the best lambda. 

```{r}
fit.lasso <- cv.glmnet(as.matrix(X_train), as.matrix(y_train), type.measure="mse", alpha=1, family="gaussian")


fitted.lasso.train <- predict(fit.lasso, newx = data.matrix(X_train), s="lambda.min")

fitted.lasso.test <- predict(fit.lasso, newx = data.matrix(X_test), s="lambda.min")

cat("Train coefficient: ", cor(as.matrix(y_train), fitted.lasso.train)[1])
cat("\nTest coefficient: ", cor(as.matrix(X_test), fitted.lasso.test)[1])

```

As we can see above, the train and test coefficients are 0.83 and -0.24.


#### Model3: Elastic Net Regression

According to machinelearningmastery.com, Elastic Net is an extension of linear regression that will add regulirization penalties to the loss function during training. 

```{r}
fit.elnet <- glmnet(as.matrix(X_train), as.matrix(y_train), family="gaussian", alpha=.5)

fit.elnet.cv <- cv.glmnet(as.matrix(X_train), as.matrix(y_train), type.measure="mse", alpha=.5,
                          family="gaussian")

fitted.elnet.train <- predict(fit.elnet.cv, newx = data.matrix(X_train), s="lambda.min")

fitted.elnet.test <- predict(fit.elnet.cv, newx = data.matrix(X_test), s="lambda.min")

cat("Train coefficient: ", cor(as.matrix(y_train), fitted.elnet.train)[1])
cat("\nTest coefficient: ", cor(as.matrix(X_test), fitted.elnet.test)[1])

```


#### Plot MSE

```{r message=FALSE, warning=FALSE}
par(mfrow=c(3,1))

plot(fit.lasso, xvar="lambda")
plot(fit.ridge, xvar="lambda")
plot(fit.elnet.cv, xvar="lambda")

```



### Model Group B: Non Linear Regression Model

Non Linear Regression models do not follow the $y=ax+b$ approach.  It allows for flexibility in the model to fit non linear trends in data.  

We will go over three examples of non linear regression: Neural Networks, K-Nearest Neighbors (KNN), Support Vector Machines (SVM), and Multivaraite Adaptive Regression Splines (MARS).


#### Model4: Neural Networks

Neural Networks are regression techniques inspired by theories of how the brain works (neurons signalling to one another).  The outcome is modeled by an input layer and an intermediary set of unobserved values, hidden units, made up of linear combinations of the original predictors.  

The parameters used in the model include number of hidden units, decay, number of iterations to find parameter estimates, and the number of paramters used by the model.


```{r}
# averaging a simple Neural Network with 5 hidden units
nnetAvg <- avNNet(X_train, y_train,
                  size = 5,
                  decay = 0.01,
                  repeats = 5,
                  linout = TRUE,
                  trace = FALSE,
                  maxit = 500,
                  MaxNWts = 5 * (ncol(X_train) + 1) +5+1)

nnetAvg
```


##### Variable Importance

```{r}
varimp <- varImp(nnetAvg)

varimp %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

##### Prediction Performance

```{r}
nnetPred <- predict(nnetAvg, newdata = X_test)
nnetPredPerf <- postResample(pred = nnetPred, obs = y_test)

nnetPredPerf['Family'] <- 'NonLinear'
```

#### Model5: K-Nearest Neighbors

The **KNN** model predicts a new sample for regression using the **K**-closest data points from the training set.  The method uses the Euclidean distance or the straight-line distance between samples.


$$(\sum^p_{j=1}(x_{aj}-x_{bj})^q)^{1/q}$$

The `train` function in the **caret** package uses the `knn` method.

```{r}
knnFit <- train(X_train, y_train,
                   method = 'knn',
                   tuneLength = 14,
                   trControl = trainControl(method = 'cv'))

knnFit$finalModel

knnFit
```

##### KNN Plot

```{r}
plot(knnFit)
```

##### Variable Importance

```{r}
varimp <- varImp(knnFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

##### Prediction Performance

```{r}
knnPred <- predict(knnFit, newdata = X_test)
knnPredPerf <- postResample(pred = knnPred, obs = y_test)

knnPredPerf['Family'] <- 'NonLinear'
```

#### Model6: Suppost Vector Machines

SVM regression models try to find a regression line where each data point is within a threshold.  The error is a parameter of the model and it will then try to minimize the coefficients used in the model. This allows the model eliminate features and have greater interpretability.

We will again use the **caret** package and apply a **radial SVM** model.  This method will allow the model to find trends in the dataset.


```{r}
svmRFit <- train(X_train, y_train,
                   method = 'svmRadial',
                   tuneLength = 14,
                   trControl = trainControl(method = 'cv'))

svmRFit
```

##### Variable Importance

```{r}
varimp <- varImp(svmRFit)

varimp$importance %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```

##### Prediction Performance

```{r}
svmRPred <- predict(svmRFit, newdata = X_test)
svmRPredPerf <- postResample(pred = svmRPred, obs = y_test)

svmRPredPerf['Family'] <- 'NonLinear'
```

#### Model7:Multivaraite Adaptive Regression Splines (MARS)

The MARS model finds *knots* to determine piece wise linear model where each new feature models an isolated portion of the data.  The predictor / cut point combination that has the smallest error is used for the model in an iterative process until a stopping point is reached.  


We will use the **earth** package to generate this MARS model.


```{r}
# mars
marsFit <- earth(X_train, y_train)
summary(marsFit)
```

##### Visualizing MARS Model

```{r}
plotmo(marsFit)
```

##### Variable Importance

```{r}
varimp <- varImp(marsFit)

varimp %>%
  arrange(desc(Overall)) %>%
  kable() %>% kable_styling(bootstrap_options = c("striped", "hover", "condensed", "responsive")) %>% scroll_box(width="100%",height="400px")
```


##### Prediction Performance

```{r}
marsPred <- predict(marsFit, newdata = X_test)
marsPredPerf <- postResample(pred = marsPred, obs = y_test)

marsPredPerf['Family'] <- 'NonLinear'
```

### Model Group C: Trees and Boosting Model



## Conclusion


## References

[1) A Study of Violent Crimes in US Communities](https://rstudio-pubs-static.s3.amazonaws.com/412546_2286f95f0fb048149c86f4ae0d39b46e.html)
